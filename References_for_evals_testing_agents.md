# Evaluating and Testing Agentic AI Systems: Recent Resources and Approaches

## Recent Resources on Agentic AI Evaluation

### Industry Guides and Tutorials:

**Galileo AI – Four New Agent Evaluation Metrics (Oct 2025):** Blog post introducing four metrics for agent performance beyond simple pass/fail. Galileo's platform adds **Agent Flow, Agent Efficiency, Conversation Quality, and Intent Change** metrics to capture multi-step behavior and user experience [1, 2]. It emphasizes *evaluation-driven development*, with IDE-integrated tools to generate test data, run evaluations, and even define **custom domain-specific metrics** in natural language [3, 4]. This resource highlights industry's push for richer metrics (e.g. measuring workflow adherence, step count, user satisfaction) to evaluate agentic AI in production.

**Deepchecks – Evaluating Agentic AI Systems in Production (Nov 2025):** An 8-minute read discussing characteristics of agentic AI and challenges of evaluating them in real-world deployments [5, 6]. It outlines *key metrics* for assessment, spanning **technical performance (task success), safety/alignment, adaptability (drift tracking), observability (tool usage, errors), and human-centric factors (user trust, cost)** [7, 8]. The article advocates multi-step testing and continuous monitoring. It also provides examples from healthcare and finance, demonstrating the need to evaluate policy adherence, error rates, and system robustness in multi-step agent workflows [9, 10].

**Maxim AI – Evaluating Agentic AI Systems: Frameworks, Metrics, and Best Practices (Nov 2025):** A concise guide proposing a **three-layer evaluation framework** [11]. It suggests measuring **System Efficiency** (latency, token usage, tool call counts) [12, 13], **Session-Level Outcomes** (end-to-end task success, trajectory quality) [12, 14], and **Node-Level Precision** (correct tool selection, step-level accuracy) within agent traces. The tutorial recommends combining automated LLM-based evaluators ("LLM-as-a-Judge") with human review, and integrating evaluation into both offline testing and online monitoring. It provides practical metrics (e.g. step utility, plan coherence, error recovery) and emphasizes context-specific metrics for specialized agents (RAG systems, voice assistants, coding agents) to ensure reliable behavior [15, 16].

**testRigor Blog – Different Evals for Agentic AI: Methods, Metrics & Best Practices (2025):** An in-depth tutorial style post on rethinking QA/testing for agentic AI. It explains why traditional one-shot accuracy metrics fail for probabilistic, multi-step agents and stresses the need to "grade their behavior" across steps [17, 18]. It covers evaluation methods: **static offline tests**, **in-the-loop dynamic evaluation** (sandbox or A/B testing with live data), **LLM-as-a-Judge programmatic evaluation** for subjective criteria [19, 20], and **human-in-the-loop review** for high-stakes cases [21]. The article enumerates common metrics for agents – **Task Success Rate, Tool Use Accuracy, Plan/Goal Adherence, Efficiency (latency), Cost (tokens), Autonomy** (steps without human aid), **Robustness** (recovery rate), and **Hallucination rate** – explaining why each matters for trust and reliability. It concludes with best practices like building comprehensive scenario test sets (including synthetic and real failure replays) and instrumenting agents for deep observability and auditability [24, 25]. This serves as a practical blueprint for QA engineers adapting testing processes to agentic AI.

**Medium Series – Evaluating Agentic AI Systems: The Complete Framework by Nitika Bhatia (Oct–Nov 2025):** A seven-part Medium series addressing the "evaluation crisis" in agentic AI. Part 1 diagnoses five critical gaps between lab evaluation and production reality – distribution mismatch, coordination failures, subjective quality assessment, lack of root cause analysis, and high variance in non-deterministic systems [26, 27]. The series proposes augmenting existing tools (LangSmith, RAGAS, DeepEval, Humanloop, etc.) with new layers and techniques to close these gaps [28, 29]. Subsequent parts (2–6) delve into solutions: e.g. **better eval data representing messy real queries, system-level integration tests for multi-agent coordination, scalable LLM-driven scoring for subjective aspects, automated failure tracing, and statistical evaluation methods** to handle run-to-run variance. The final Part 7 synthesizes a comprehensive framework and roadmap for deploying evaluation in production. This series provides a deep conceptual look at why standard ML evaluation breaks for agents and how practitioners can extend evaluation **beyond single-step, deterministic paradigms** to capture the complexity of autonomous agents [30, 31].

### Research Papers and White Papers:

**Shukla (EngrXiv/SSRN Preprint, Aug 2025) – Evaluating Agentic AI Systems: A Balanced Framework:** A 9-page white paper arguing that current evaluations (focusing only on task success or latency) overlook *sociotechnical dimensions* critical for autonomous agents [32]. It proposes a **balanced evaluation framework spanning five axes**: (1) **Capability & Efficiency**, (2) **Robustness & Adaptability**, (3) **Safety & Ethics**, (4) **Human-centered Interaction**, and (5) **Economic & Sustainability impacts** [33]. The paper introduces novel metrics like *"goal-drift score"* to detect when an agent strays from its original goal, and *harm-reduction indices* for safety compliance [34]. Case studies from industry deployments are used to illustrate how agentic AI can yield productivity gains but often *omit measures of fairness, trust, or long-term sustainability* [35]. The author calls for **multidimensional evaluation** that combines automated metrics, human-in-the-loop scoring, and even economic analysis, to ensure responsible adoption of agentic AI [36]. This work provides an academic perspective on broadening metrics beyond purely technical performance.

**Akshathala et al. (arXiv Dec 2025) – Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems:** A forthcoming academic paper (for AGENT'26) highlighting that many current evaluations rely on "binary task completion" and miss the uncertainty and nondeterminism inherent to LLM-based agents [37, 38]. Based on an industry deployment with a cloud operations (CloudOps) agent, the authors identify practical gaps and propose an **end-to-end Agent Assessment Framework with four pillars: LLM, Memory, Tools, and Environment** [39]. This framework assesses an agent's tool use (can it invoke the right tools correctly?), memory usage (does it store/retrieve information effectively?), interaction with its environment, and base LLM performance in an integrated way. Experiments on a cloud automation task show that this method catches behavioral deviations (like tool misuse or memory failures) that simple success/fail metrics overlooked [40, 41]. The paper underscores that evaluating agents requires examining *intermediate competencies* (planning quality, tool interplay, etc.) rather than just final outputs, aligning with the idea of **layer-by-layer evaluation**.

**RAGAS Framework (Es et al., revised Apr 2025):** Although focused on a subset of agentic systems (Retrieval-Augmented Generation), the RAGAS paper is a relevant resource for custom evaluation metrics. It presents **reference-free metrics** to assess RAG pipelines along multiple dimensions: how well the retrieval component finds relevant context, how faithfully the LLM uses those contexts, and the quality of the final generated answer [42]. Updated in 2025, RAGAS provides an open-source toolkit and metrics (some LLM-based) to automatically evaluate RAG without human-labeled answers. This specialization exemplifies how *custom metrics can be defined for specific agent behaviors* (like checking if an answer is grounded in retrieved documents). It's an example of the community extending evaluation beyond generic accuracy, tailored to complex LLM-tool systems.

**GAIA and AgentBench Benchmarks (2024–2025):** These are large-scale benchmark suites to systematically evaluate agentic AI in research. **GAIA (General AI Assistant)**, introduced by Meta AI in late 2023, comprises tasks requiring tool use, web browsing, multi-modal reasoning, and multi-turn dialogue [44]. It aims to test if an AI assistant can perform complex, realistic tasks and has a public leaderboard [45]. **AgentBench** (THUDM, ICLR 2024) evaluates LLM-as-agent across 8 different environments, from web navigation to coding, measuring reasoning, decision-making, and tool-use in open-ended contexts [46]. While primarily research-oriented, these benchmarks provide standardized task-based evaluations where an agent's overall success on complex tasks can be compared across models. They also often log intermediate actions, offering insight into behavioral failures (e.g. an agent might get stuck in a loop on a web task – highlighting a evaluation point beyond "solved or not"). These resources underscore the push for **common evaluation tasks and leaderboards** for agentic AI, complementing custom in-house metrics with community benchmarks.

### Courses and Training:

**Stanford University – CS329T: Trustworthy Machine Learning (Autumn 2025):** This graduate seminar includes a module on **Building and Evaluating Agentic Systems** [47]. Through lectures and labs, it covers how traditional model evaluation must be adapted for agents, addressing topics like multi-agent coordination, safety testing, and human feedback for agents. The fact that Stanford and other universities (e.g. a Berkeley RDI course on Agentic AI) have dedicated curriculum on agent evaluation reflects the emerging best practices. Although course materials are not fully public, slides and recorded lectures (e.g. Stanford's "LLM Evaluation" lecture [48]) serve as educational tutorials on metrics and pitfalls. These courses consolidate current research into teachable principles – for example, stressing evaluation as a continuous process (from prototyping to deployment) and teaching how to design new eval tests for agent behaviors. Students learn to define what "good" means for an agent and measure it, which is directly relevant to practitioners defining custom metrics.

**ODSC AI Summit Workshops (2025):** The Open Data Science Conference ran workshops and talks on evaluation. One highlighted session was **"Mastering AI Evaluation in 2025" with Ian Cairns (Freeplay)** [49, 50], which shared lessons on taking LLM agents from prototype to production with robust testing and observability. The associated Ai X Podcast episode "The Hardest Problem in AI: Evaluation in 2025" reinforced strategies such as starting evaluation early, mapping out failure modes, and using both **offline tests and online monitoring** to ensure reliability [51, 52]. Training sessions like "Practical Guide to LLM Evaluation" (Michelle Yi, ODSC West 2025) were also featured [53]. These industry-focused courses act as tutorials for engineers to implement evaluation pipelines. They often include hands-on practice with frameworks (OpenAI Evals, DeepEval, etc.) and demonstrate how to automate evaluations (for example, how to integrate tests into CI/CD for LLMs). The key takeaway is an emphasis on **"evaluation-driven development"**: treating agent evaluation as a first-class part of the development cycle, not a one-time task [52, 54].

### Podcasts and Talks:

**ODSC "Ai X" Podcast – The Hardest Problem in AI: Evaluation in 2025 (Sept 2025):** A 45-minute discussion with Ian Cairns (Freeplay CEO) focusing on evaluation challenges for LLM-based systems. The conversation highlights the gap between "vibe prompting" prototypes and scalable, reliable AI systems [55, 52]. Cairns describes how top teams at companies like OpenAI approach evaluation by first defining **"what good looks like"** for their specific application, then creating both **offline test sets and online metrics** to monitor those quality targets [56, 54]. Notably, he advocates mixing automated metrics with **regular human review** of outputs, since subtle issues in tone or UX often slip past numeric scores [57]. The podcast also covers evaluation tooling in practice: e.g. using telemetry to catch regressions, and layering **unit tests for prompts/tools and end-to-end tests for agent tasks** [58, 59]. It's a valuable resource for hearing real-world experiences in deploying evaluation harnesses for agentic AI, emphasizing cultural shifts like dedicating "evaluation engineers" and building a quality-driven mindset [60, 61].

**TWIML AI Podcast – AI Agents and Multi-Agent Systems (Trends 2025):** In this episode (with guest Victor Dibia, 2025), broader trends in agentic AI are discussed, including evaluation. Dibia notes that as agents get deployed, **continuous evaluation and anomaly detection** become essential to treat "agent behavior like system health metrics" [62]. While not solely focused on evaluation, the podcast provides context on industry expectations: that as every software product gains AI agents, robust testing and monitoring will be a key part of their maintenance. Listeners gain insight into emerging best practices such as sandbox testing multi-agent interactions and using simulation environments to test agent decision-making safely before real-world release.

**TechnologIST (Institute for Security and Technology) – The Coming Age of Agentic AI (May 2025):** A panel podcast with experts like Dr. Margaret Mitchell discussing the implications of agentic AI. Part of the conversation touches on **evaluation for safety and ethics**, given Mitchell's AI ethics background. They highlight the need for behavioral testing of autonomous agents – for example, stress-testing an agent with adversarial inputs or unexpected scenarios to see if it stays aligned with ethical guidelines. This complements technical evaluations with a more societal lens. For those developing agentic AI in sensitive domains, the takeaway is to incorporate **red-teaming, bias audits, and scenario-based evaluations** into the test suite, beyond functional metrics. (Note: this podcast doesn't provide a concrete framework but reinforces why safety evaluation is integral, aligning with the multi-dimensional frameworks proposed in research.)

### Open-Source Frameworks and Repositories:

**OpenAI Evals** (GitHub: `openai/evals`, updated 2025): An open-source framework and registry for evaluating LLMs and LLM-based systems [63]. It provides a structured way to create custom "evals", essentially test cases or benchmarks, that can be run to compare model performance on specific tasks. OpenAI Evals comes with a library of predefined evaluations (covering things like math, coding, factual Q&A, etc.) and allows writing new evals for custom use cases [63]. Importantly, it supports **user-defined metrics and checkers**, meaning developers can program their own evaluation logic or success criteria. For agentic AI, one could, for example, write an eval that simulates a multi-step task (with a sequence of prompts and tool calls) and then define a custom metric to grade the agent's overall success or efficiency. The framework also integrates with the OpenAI API and has a dashboard, facilitating automated testing as part of an LLMOps pipeline. In summary, OpenAI Evals is a popular starting point for teams to build **reproducible, shareable evaluation suites**, and its design encourages community contributions of new evaluation methods.

**DeepEval (Confident AI) – Open-Source LLM Evaluation Library:** DeepEval is both a library and a set of guides/docs for granular agent evaluation. It treats an AI agent's run as an **execution trace** (sequence of reasoning steps and tool actions) and provides metrics to evaluate each part of that trace [64]. For example, DeepEval includes metrics like `PlanQualityMetric` and `PlanAdherenceMetric` to score the agent's planning layer [65, 66], and `ToolCorrectnessMetric` or `StepEfficiencyMetric` for the action/execution layer [67, 68]. These metrics use LLM judges under the hood to evaluate aspects like coherence or correctness of each step. A highlight is the ease of **defining custom metrics**: DeepEval provides a `BaseMetric` class that developers can subclass to implement their own evaluation logic (including optional use of an LLM for scoring) [66]. The documentation walks through building a custom metric – for instance, combining multiple existing metrics or evaluating something domain-specific – and how to integrate it into a CI pipeline [69, 70]. DeepEval also supports trace instrumentation (similar to logging each agent action) and can integrate with observability platforms. With its focus on agent layers and customizability, DeepEval is a leading tool to **pinpoint failure modes** in complex agents by testing reasoning and tool-use separately [71, 72]. Many teams use it to automate fine-grained testing and to catch regressions in agent behavior with each code/model change.

**LangSmith (LangChain) and Other LLMOps Tools:** LangSmith is an evaluation and tracing platform provided by LangChain, widely used by developers building LLM agents. It enables capturing detailed traces of agent executions (all intermediate LLM calls and tool calls) and can apply **LLM-based evaluators** on the outputs to give quality scores [75]. LangSmith's appeal is that it ties into the development stack – one can run experiments with different prompts or model versions and immediately see how they score on custom evaluation criteria (like accuracy, relevance, etc.). Similarly, **Humanloop** and **Arize AI (Phoenix)** are platforms that offer test set management, version comparisons, and monitoring dashboards [76]. They often let users define custom evaluation metrics and employ human feedback loops (e.g. rating outputs) to continually measure an agent's performance in production. These tools are popular in industry for implementing **continuous evaluation** — for example, a Humanloop or Arize dashboard might track an agent's success rate week over week, flagging any dips or anomalies. They align with best practices of integrating evaluation into the ML lifecycle (much like MLflow for traditional models, which now has an evaluation component [77]).

**RAGAS** (GitHub: `vibrantlabsai/ragas`): The code repository for RAGAS provides an **evaluation toolkit specialized for Retrieval-Augmented Generation**. It includes ready-made metrics such as context relevance, factual consistency, and answer correctness without ground truth [78, 79]. Users can plug in their own LLM as a judge or use the default. RAGAS can be seen as an example of a domain-specific evaluation framework – while not general-purpose for all agents, it demonstrates how to automate evaluation for agents that rely on external knowledge. It's relevant to agentic AI (especially QA or assistant agents that use retrieval) and serves as a blueprint for building similar custom evaluators in other domains.

**AgentBench & GAIA Repositories:** The GitHub repo `THUDM/AgentBench` contains the code, tasks, and scoring scripts for the AgentBench benchmark [80]. Similarly, GAIA's HuggingFace space [45] provides an interactive leaderboard and evaluation harness. These repos are valuable for those looking to **benchmark their agent's capabilities** against a standard: one can run their agent through these tasks to see how it fares on, say, web navigation or tool use tasks compared to published models. They also often include simulators or wrappers to interface an arbitrary agent with the tasks (for example, providing a unified API for the agent to receive task instructions and return actions). This makes them useful for testing custom agents in a controlled setting. While these are more about comparison and benchmarking than custom metrics, they enrich the evaluation toolkit by covering **task-based evaluations** under realistic conditions.

## Summary of Evaluation Approaches and Best Practices

### Dimensions of Evaluation

Across these resources, a consensus emerges that evaluating agentic AI must be **multi-dimensional**, covering both *outcome and process*. Unlike a single-step model where one might only check accuracy against a label, an agent must be evaluated on whether it accomplishes the task and how it behaves while doing so [22, 23]. Key dimensions include:

- **Task/Goal Completion** – Is the agent ultimately successful in achieving the user's goal? (This could be measured by task success rate or achievement of each required sub-task [81].)

- **Intermediate Behavior Quality** – Are the agent's reasoning steps and actions sound? This involves metrics for **planning quality, plan adherence** (does it follow its intended steps? [66, 68]), **tool use correctness** (choosing the right tool with correct parameters [82, 83]), and **efficiency** (avoiding unnecessary loops or steps [84, 85]).

- **Safety and Reliability** – Does the agent avoid harmful or rule-breaking behavior? Is it robust to errors? Measures here include **safety rule adherence, hallucination rate** (making false statements [86]), **recovery rate** from failures [87], and compliance with ethical or policy constraints [33]. Reliability also covers consistency: due to the non-determinism of LLMs, running an agent multiple times might yield different outcomes, so evaluating variance and establishing confidence intervals is recommended (as noted in the Medium series: incorporate statistical sampling to handle stochastic behavior [88]).

- **User Experience & Human Factors** – This evaluates the quality of interaction. For example, **conversation quality** (is the dialogue helpful and coherent? [2]), response tone and clarity, turn-taking behavior for voice agents [89, 90], and ultimately **user satisfaction** (which might be measured via human ratings or proxy signals). Human-centered metrics recognize that an agent's value lies not just in *what* it does but *how* it does it for the end-user [8, 91].

- **Efficiency & Cost** – Agentic systems introduce new costs (e.g. API calls, longer runtimes). Evaluations often include **latency** (end-to-end and per step) [92], **token usage** (which correlates with cost) [93], and general resource usage. An agent that solves a task but takes 15 minutes and thousands of tokens might be unacceptable compared to one that finds a solution in 5 steps. Thus, defining acceptable efficiency thresholds is part of evaluation.

- **Adaptability & Robustness** – Because agents operate in dynamic environments, tests for how well an agent adapts to changes are crucial. This could mean evaluating on perturbed scenarios (e.g. slightly different inputs or tool outputs) to see if performance degrades, tracking **drift** over time (does the agent's success rate drop as real-world data shifts? [94]), or deliberately introducing errors (like a tool failure) to see if the agent can recover. Robustness testing overlaps with reliability, aiming to ensure the agent can handle the unexpected gracefully [95, 96].

### Types of Evaluations

Several *approaches* to testing agentic AI have been identified:

**Static Task-based Evaluation:** Similar to traditional ML tests, these use a fixed dataset of tasks or queries and expected outcomes. For an agent, a task might be a complex user request (with a known correct outcome or a rubric to judge it). Benchmarks like AgentBench and GAIA fall here – they define tasks and success criteria, then score the agent on whether it completes them [46, 97]. Task-based evals are good for measuring capability: can the agent in principle do X? However, as noted, they often reduce success to a binary outcome, so they should be complemented with richer analysis. Still, they are valuable for **comparative evaluation** (e.g. to compare two agent architectures on the same tasks) and for **regression testing** (re-running the suite after updates to catch degradations).

**Behavioral & Process Evaluation:** This focuses on *how* the agent behaves, not just final success. It includes **step-by-step trace inspection** and metrics applied to each part of the agent's reasoning or execution. For example, one might simulate an agent's entire process and then evaluate the coherence of its plan, the correctness of each action, whether it properly used information from memory, etc. Frameworks like DeepEval explicitly enable this by evaluating reasoning and action layers separately [73, 74]. Behavioral evaluation can also be scenario-driven: testing how the agent behaves under certain conditions (edge cases, tricky inputs, adversarial prompts). This is akin to unit testing for different behaviors (e.g. "Does the agent know to stop trying a tool after N failures?" or "Will the agent refuse a request that violates policy?"). It often requires crafting specific test scenarios or using model-in-the-loop judges to assess behavior against norms.

**Safety and Reliability Testing:** Here the aim is to probe the agent's adherence to safety constraints and its resilience. This can involve **red-team testing** (intentionally trying to get the agent to produce forbidden content or make a dangerous decision) [21]. It can also involve stress-testing with malformed inputs or high-load situations. Reliability tests might include running the agent many times on the same task to see how variable the outcomes are, checking if it sometimes fails or behaves erratically. If an agent interacts with external systems (databases, APIs), reliability testing would simulate those systems failing or returning unexpected data, to see if the agent can handle it without crashing or corrupting data [98, 99]. Such evaluations ensure **robust performance and alignment** before deployment in high-stakes settings. Notably, human oversight often remains part of safety evaluation – for instance, having humans review random samples of the agent's outputs or decision logs to catch subtle issues (this practice is recommended in many resources [100, 57]).

**Online and Continuous Evaluation:** A strong theme is that evaluation doesn't stop at deployment; it must be automated and ongoing in production (often referred to as **"evaluation-driven monitoring"**). This type includes **A/B testing** (comparing a new agent version against the current one on live traffic in a controlled way [95, 101]), **canary releases** with metric monitoring, and real-time anomaly detection on agent outputs (flagging if the agent's behavior deviates from normal patterns). Tools like Arize or custom dashboards track metrics like success rate, error rate, latency in production and can trigger alerts if thresholds are breached [102, 103]. Moreover, **feedback loops** are employed: for example, collecting user feedback signals (like thumbs-up/down, or follow-up corrections by users) and treating those as labels to evaluate and retrain the agent. Continuous evaluation ensures that as the environment or the model changes, the agent remains effective and safe. It shifts evaluation from a one-time QA phase to a **CI/CD-like process for AI**, where every update is tested and the system is continuously health-checked.

### Defining Custom Metrics

A crucial aspect of evaluating agentic AI is deciding *what to measure* for your specific agent. The resources consistently advise teams to **define success criteria up front** – essentially, determine the key metrics that correlate with a "good" performance for your application [52, 54]. This often means creating **custom evaluation metrics** beyond generic ones:

- **Custom metrics might be composite.** For instance, a support chatbot agent's quality might be a weighted combination of factual accuracy, appropriate tone, and task completion. Tools like Galileo allow defining such metrics in plain language (their platform can translate a natural language description into an LLM-based metric) [104]. DeepEval allows coding metrics to combine others (e.g. require both correctness and succinctness) [70, 105].

- **Domain-specific metrics are encouraged.** If you have an agent that writes Python code, you might evaluate "percentage of generated scripts that run without error". If your agent plans travel itineraries, you might measure "plan feasibility" or cost-efficiency of the plan. Manish Shukla's framework even suggests goal-drift and economic sustainability metrics [106] – these are highly context-dependent but ensure evaluation aligns with business or ethical goals, not just technical goals.

- **Implementing custom metrics has been made easier by open libraries.** OpenAI Evals, for example, provides a template to plug in custom Python code to judge an output [107, 108]. If a team cares about, say, an agent's *politeness*, they can write a small function or prompt that scores the assistant's response for politeness and integrate that into their evals. Likewise, LangChain's evaluators or PromptFoo allow writing a custom prompt that an LLM will use to grade outputs (effectively a custom metric via AI judge). The key is that teams are **empowered to formalize quality criteria** that matter to them and measure those consistently.

### Popular Frameworks and Tools

As summarized in the testRigor blog's tool table [102, 109] and our resource list, the ecosystem for evaluating agentic AI is rich:

- **For observability and tracing:** LangSmith (from LangChain) is widely used, as it captures agent execution details and offers built-in evaluation. Tools like Helicone or Phoenix (Arize) serve similar purposes, logging interactions for later analysis.

- **For automated evaluation harnesses:** DeepEval (open-source) and OpenAI Evals are prominent. They cater to developers writing tests and metrics in code. PromptFoo is another open-source tool to quickly A/B test prompts or agents with different inputs and get LLM-judged evaluations. MLflow's Evaluate module is bridging into LLM evals for those already using MLflow in pipelines [110].

- **For benchmarking:** apart from AgentBench and GAIA, others like WebArena (a benchmark for web-based agent tasks) or SWE-Bench (for software engineering agents) exist. These are more for research benchmarking but often drive what metrics become standard.

- **For monitoring in production:** Traditional APM/monitoring tools are integrating AI evals. The blog mentions Dynatrace, PostHog along with Arize [111]. These might track performance metrics of agents in apps, akin to tracking microservice health. Additionally, specialized platforms like Humanloop or Braintrust provide interfaces to manage evaluation datasets, run evaluations periodically, and involve humans when needed [112, 76].

- **For specific use-cases:** RAGAS for RAG, as discussed; evals for multi-agent sims (e.g., Arena for multi-agent interactions); and emerging safety eval tools (OpenAI has "red team at scale" tools internally, and academia has adversarial testing suites).

In terms of popularity, OpenAI Evals has a large community due to open-source release and being model-agnostic, and LangSmith is popular among those using LangChain. DeepEval is gaining traction for fine-grained testing, especially when developers need to write their own metrics or integrate with CI. On the enterprise side, platforms like Arize, Monte Carlo, WhyLabs etc., while originally for monitoring models, are extending to LLM agents (e.g. monitoring output distributions, detecting anomalies).

### Main Takeaways

Evaluating agentic AI systems is more complex than evaluating a static model – it requires **new metrics, layered methods, and continuous processes**. Good evaluation strategies combine task-based tests (can the agent do what it's supposed to?) with behavioral analysis (how is it doing it?) and ensure safety and reliability checks are in place for the agent's autonomy. The community is actively developing both conceptual frameworks (balanced scorecards, multi-axis evaluations [33]) and practical tools (from open-source libraries to enterprise platforms) to support this. A recurring theme is the need for **automation with human oversight**: leverage AI to judge AI where possible (for scale), but keep humans in the loop for subjective or high-impact judgments [113, 21]. Finally, integrating these evaluations into the development and deployment pipeline – much like writing unit tests and monitoring dashboards for traditional software – is becoming standard practice for agentic AI. This ensures that as AI agents take on more complex tasks and greater autonomy, we can trust and verify their performance on all the facets that matter, from correctness and efficiency to safety and user satisfaction.

**Sources:** The information above was synthesized from a range of recent resources, including industry blog posts [1, 3], whitepapers [32], documentation guides [66], and expert interviews [56], all published within the last 8 months. These sources provide further detail and examples for readers interested in implementing agent evaluation in practice.

---

## References

[1, 2, 3, 4, 84, 85, 104] [Four New Agent Evaluation Metrics](https://galileo.ai/blog/four-new-agent-evaluation-metrics)

[5, 6, 7, 8, 9, 10, 91, 94] [Evaluating Agentic AI Systems in Production | Deepchecks](https://www.deepchecks.com/evaluating-agentic-ai-systems-production/)

[11, 12, 13, 14, 15, 16, 92] [Evaluating Agentic AI Systems: Frameworks, Metrics, and Best Practices](https://www.getmaxim.ai/articles/evaluating-agentic-ai-systems-frameworks-metrics-and-best-practices/)

[17, 18, 19, 20, 21, 22, 23, 24, 25, 75, 77, 81, 86, 87, 93, 95, 96, 101, 102, 103, 109, 110, 111] [Different Evals for Agentic AI: Methods, Metrics & Best Practices - testRigor AI-Based Automated Testing Tool](https://testrigor.com/blog/different-evals-for-agentic-ai/)

[26, 27, 28, 29, 30, 31, 76, 88, 112] [Part 1 — Evaluating Agentic AI Systems: The Complete Framework | by Nitika Bhatia | Oct, 2025 | Medium](https://medium.com/@nitikab23/the-five-evaluation-gaps-killing-your-agentic-ai-in-production-6796cae4a0a1)

[32, 33, 34, 35, 36, 106] [Evaluating Agentic AI Systems: A Balanced Framework for Performance, Robustness, Safety and Beyond by Manish Shukla :: SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5402054)

[37, 38, 39, 40, 41] [[2512.12791] Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://www.arxiv.org/abs/2512.12791)

[42, 43] [[2309.15217] Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)

[44] [GAIA: a benchmark for general AI assistants | Research - AI at Meta](https://ai.meta.com/research/publications/gaia-a-benchmark-for-general-ai-assistants/)

[45] [GAIA Leaderboard - a Hugging Face Space by gaia-benchmark](https://huggingface.co/spaces/gaia-benchmark/leaderboard)

[46] [AgentBench (AgentBench) - Agentic Design | Agentic Design Patterns](https://agentic-design.ai/patterns/evaluation-monitoring/agentbench)

[47] [CS 329T | Syllabus - Stanford University](https://web.stanford.edu/class/cs329t/syllabus.html)

[48] [Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 8](https://www.youtube.com/watch?v=8fNP4N46RRo)

[49, 50, 53] [The ODSC AI West 2025 Preliminary Schedule, Mastering AI Evaluation, Building Real World Agentic Applications, and GPT-5 News. | by ODSC - Open Data Science | Medium](https://odsc.medium.com/the-odsc-ai-west-2025-preliminary-schedule-mastering-ai-evaluation-building-real-world-agentic-97c321019b78)

[51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 89, 90, 100, 113] [Lessons from Ian Cairns on Mastering AI Evaluation](https://opendatascience.com/mastering-ai-evaluation-in-2025-lessons-from-ian-cairns-on-building-reliable-systems/)

[62, 98, 99] [The State of Agentic AI in 2025: A Year-End Reality Check](https://www.arionresearch.com/blog/the-state-of-agentic-ai-in-2025-a-year-end-reality-check)

[63, 107] [GitHub - openai/evals: Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.](https://github.com/openai/evals)

[64, 73, 74] [AI Agent Evaluation | DeepEval - The Open-Source LLM Evaluation Framework](https://deepeval.com/guides/guides-ai-agent-evaluation)

[65, 66, 67, 68, 82, 83] [AI Agent Evaluation Metrics | DeepEval - The Open-Source LLM Evaluation Framework](https://deepeval.com/guides/guides-ai-agent-evaluation-metrics)

[69, 70, 71, 72, 105] [Building Custom LLM Metrics | DeepEval - The Open-Source LLM Evaluation Framework](https://deepeval.com/guides/guides-building-custom-metrics)

[78] [Evaluation with Ragas - Medium](https://medium.com/@danushidk507/evaluation-with-ragas-873a574b86a9)

[79] [vibrantlabsai/ragas: Supercharge Your LLM Application Evaluations](https://github.com/vibrantlabsai/ragas)

[80] [THUDM/AgentBench: A Comprehensive Benchmark to Evaluate ...](https://github.com/THUDM/AgentBench)

[97] [10 AI agent benchmarks - Evidently AI](https://www.evidentlyai.com/blog/ai-agent-benchmarks)

[108] [evals/docs/build-eval.md at main · openai/evals - GitHub](https://github.com/openai/evals/blob/main/docs/build-eval.md)
