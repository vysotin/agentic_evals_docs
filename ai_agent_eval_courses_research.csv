Subject,Course Name,Platform,Instructor,Course URL,Annotation,Course Type,Key Topics,Relevance Score,Error
Online courses on AI agent evaluation and testing Coursera Udemy,Evaluating AI Agents,Udemy,Yash Thakker,https://www.udemy.com/course/evaluating-ai-agents/,"This course focuses on mastering quality, performance, and cost evaluation frameworks for LLM agents using tools like Patronus and LangSmith. It covers designing comprehensive evaluation metrics, implementing logging systems, conducting A/B testing, and setting up production monitoring dashboards to track agent performance over time. The course is highly dedicated to the evaluation and testing aspects of AI agents.",dedicated,"LLM agent evaluation, performance metrics, cost evaluation, logging systems, A/B testing, production monitoring, LangSmith, Patronus, PromptLayer",10,
LLM evaluation courses online 2024 2025 2026,LLM evaluations for AI product teams,Evidently AI,Elena Samuylova,https://www.evidentlyai.com/llm-evaluations-course,"This is a free 7-day email course that provides a gentle introduction to evaluating LLM-powered products. It covers evaluation methods, benchmarks, LLM guardrails, and creating custom LLM judges, designed for AI product managers and leaders.",dedicated,"LLM evaluation methods, benchmarks, LLM guardrails, custom LLM judges, production monitoring, evaluation datasets, synthetic data, hallucinations, prompt injection, jailbreaks, LLM observability, tracing, evals, RAGs, QA systems, agents, predictive metrics",10,
AI agent testing courses edX Pluralsight,Evaluating AI Agents,DeepLearning.AI,"John Gilhuly, Aman Khan (DeepLearning.AI in partnership with Arize AI)",https://www.deeplearning.ai/short-courses/evaluating-ai-agents/,"This course teaches how to systematically assess and improve AI agent performance. It covers adding observability, setting up evaluations for agent components, and structuring evaluations into experiments to iterate and improve output quality.",dedicated,"Observability, debugging AI agents, evaluation setup, code-based evaluators, LLM-as-a-Judge, metrics, structured experiments, improving agent performance, tracing AI agents, monitoring agents in production",10,
Agentic AI evaluation training courses online,Evaluating AI Agents,DeepLearning.AI,"John Gilhuly, Aman Khan",https://www.deeplearning.ai/short-courses/evaluating-ai-agents/,"This course focuses on systematically assessing and improving AI agent performance through structured evaluation processes. It covers building an AI agent, adding observability for debugging, and component-wise evaluation to refine performance. The course emphasizes the importance of evaluations in refining AI agent performance systematically.",dedicated,"AI agent evaluation, AI agent testing, observability, debugging, performance metrics, LLM-as-a-Judge, structured experiments, production monitoring, agent architecture",10,
Machine learning evaluation courses for AI agents,Evaluating AI Agents,DeepLearning.AI,"John Gilhuly, Aman Khan",https://www.deeplearning.ai/short-courses/evaluating-ai-agents/,"This course teaches how to systematically assess and improve AI agent performance. It covers adding observability, setting up evaluations for agent components, and structuring evaluations into experiments to iterate and improve output quality.",dedicated,"Observability, debugging, evaluation setup, testing examples, evaluators (code-based, LLM-as-a-Judge), metrics, structured experiments, improving agent performance, tracing AI agents, monitoring agents in production.",10,
LLM observability and monitoring courses online,LLM evaluations for AI builders,Evidently AI,Evidently AI,https://www.evidentlyai.com/courses,"This free video course from Evidently AI provides 10 hands-on code tutorials for LLM evaluations. It covers designing custom LLM judges, RAG evaluations, and adversarial testing, making it suitable for AI builders and product teams with basic Python skills.",dedicated,"LLM evaluation, custom LLM judges, RAG evaluations, adversarial testing, production LLM monitoring, AI observability",10,
AI agent quality assurance courses certification,Evaluating AI Agents,DeepLearning.AI,"John Gilhuly, Aman Khan",https://www.deeplearning.ai/short-courses/evaluating-ai-agents/,"This course teaches how to systematically assess and improve AI agent performance. It covers adding observability, setting up evaluations for agent components using various evaluators, and structuring experiments to refine output quality and agent behavior.",dedicated,"Observability, Debugging AI agents, Evaluation setup, Testing examples, Code-based evaluators, LLM-as-a-Judge, Human annotations, Metrics, Structured experiments, Improving output quality, Tracing AI agents, Monitoring agents in production, LLM-based systems evaluation, Traditional software testing, AI agent structure, Routers, Skills, Memory, Convergence score",10,
Production AI evaluation courses online learning,AI Evals Certification,Product School,Product School,https://productschool.com/certifications/ai-evals,"This certification course from Product School focuses on equipping Product Managers with frameworks and playbooks for AI evaluation. It covers designing evaluation suites to surface hidden risks, integrating gating mechanisms into CI/CD pipelines, and scaling evaluations across products to build trusted AI products.",dedicated,"AI evaluation metrics, failure mode discovery, systematic evaluation suites, eval gates, scaling AI evaluations, responsible AI practices",10,
Generative AI evaluation and testing courses,Evaluating and Debugging Generative AI Models Using Weights and Biases,DeepLearning.AI,Carey Phelps,https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai/,"This course introduces Machine Learning Operations tools, specifically the Weights & Biases platform, to manage the workload of diverse data sources, vast data volumes, model and parameter development, and conducting numerous test and evaluation experiments in ML and AI projects. It teaches how to instrument a training notebook, add tracking, versioning, and logging, and implement monitoring and tracing of LLMs over time in complex interactions.",dedicated,"LLM evaluation, generative image model evaluation, platform-independent tools, instrumenting training notebooks, tracking, versioning, logging, monitoring, tracing LLMs, Weights & Biases platform, hyperparameter config, run metrics, dataset and model versioning, experiment results, tracing prompts and responses",10,
AI safety and evaluation courses online platforms,Evaluating AI Agents,DeepLearning.AI,"John Gilhuly, Aman Khan (in partnership with Arize AI)",https://www.deeplearning.ai/short-courses/evaluating-ai-agents/,"This course focuses on systematically assessing and improving AI agent performance. It covers structured evaluation processes to refine agent performance during development and in production, moving beyond trial and error.",dedicated,"AI agent evaluation, AI agent testing, AI agent monitoring, observability, debugging, evaluation metrics, LLM-as-a-Judge, red-teaming, prompt engineering for evaluation, agent component evaluation, end-to-end evaluation, structured experiments, performance improvement",10,
